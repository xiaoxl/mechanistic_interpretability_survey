@Article{Elhage2021,
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal = {Transformer Circuits Thread},
  title   = {A Mathematical Framework for Transformer Circuits},
  year    = {2021},
  url     = {https://transformer-circuits.pub/2021/framework/index.html},
}

@Article{Zhang2021,
  author    = {Zhang, Yu and Tino, Peter and Leonardis, Ales and Tang, Ke},
  journal   = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  title     = {A Survey on Neural Network Interpretability},
  year      = {2021},
  issn      = {2471-285X},
  month     = oct,
  number    = {5},
  pages     = {726–742},
  volume    = {5},
  doi       = {10.1109/tetci.2021.3100641},
  file      = {:papers/2012.14261v3.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  url       = {http://dx.doi.org/10.1109/TETCI.2021.3100641},
}

@InProceedings{Gantla2025,
  author    = {Gantla, Sandeep Reddy},
  booktitle = {2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)},
  title     = {Exploring Mechanistic Interpretability in Large Language Models: Challenges, Approaches, and Insights},
  year      = {2025},
  pages     = {1-8},
  doi       = {10.1109/ICDSAAI65575.2025.11011640},
  keywords  = {Surveys;Technological innovation;Large language models;Scalability;Transformers;Safety;Complexity theory;Integrated circuit modeling;Integrated circuit reliability;Biological neural networks;Mechanistic interpretability;deep learning;neural networks;transformers;interpretability;AI;challenges;future directions},
}

@Article{Ranaldi2025,
  author    = {Ranaldi, Leonardo},
  journal   = {Big Data and Cognitive Computing},
  title     = {Survey on the Role of Mechanistic Interpretability in Generative AI},
  year      = {2025},
  issn      = {2504-2289},
  month     = jul,
  number    = {8},
  pages     = {193},
  volume    = {9},
  doi       = {10.3390/bdcc9080193},
  file      = {:papers/BDCC-09-00193.pdf:PDF},
  publisher = {MDPI AG},
}

@Article{Golgoon2024,
  author        = {Golgoon, Ashkan and Filom, Khashayar and Kannan, Arjun Ravi},
  journal       = {5th ACM International Conference on AI in Finance (ICAIF 2024)},
  title         = {Mechanistic interpretability of large language models with applications to the financial services industry},
  year          = {2024},
  month         = nov,
  pages         = {660--668},
  abstract      = {Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and $11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant role in the task completion.},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the 5th ACM International Conference on AI in Finance},
  collection    = {ICAIF ’24},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2024-07-15},
  doi           = {10.1145/3677052.3698612},
  eprint        = {2407.11215},
  file          = {:papers/Golgoon2024 - Mechanistic Interpretability of Large Language Models with Applications to the Financial Services Industry.pdf:PDF:http\://arxiv.org/pdf/2407.11215v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Engineering / Finance / Science (cs.CE), Computation and Language (cs.CL), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Mathematics, I.2.7, 68T01},
  primaryclass  = {cs.LG},
  publisher     = {ACM},
  series        = {ICAIF ’24},
}

@Article{Bereska2024,
  author        = {Bereska, Leonard and Gavves, Efstratios},
  title         = {Mechanistic Interpretability for AI Safety -- A Review},
  year          = {2024},
  month         = apr,
  abstract      = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2404.14082},
  eprint        = {2404.14082},
  file          = {:papers/Bereska2024 - Mechanistic Interpretability for AI Safety a Review.pdf:PDF:http\://arxiv.org/pdf/2404.14082v3},
  keywords      = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2404.14082},
}

@Article{Lin2025,
  author        = {Lin, Zihao and Basu, Samyadeep and Beigi, Mohammad and Manjunatha, Varun and Rossi, Ryan A. and Wang, Zichao and Zhou, Yufan and Balasubramanian, Sriram and Zarei, Arman and Rezaei, Keivan and Shen, Ying and Yao, Barry Menglong and Xu, Zhiyang and Liu, Qin and Zhang, Yuxiang and Sun, Yan and Liu, Shilong and Shen, Li and Li, Hongxuan and Feizi, Soheil and Huang, Lifu},
  title         = {A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models},
  year          = {2025},
  month         = feb,
  abstract      = {The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2502.17516},
  eprint        = {2502.17516},
  file          = {:papers/Lin2025 - A Survey on Mechanistic Interpretability for Multi Modal Foundation Models.pdf:PDF:http\://arxiv.org/pdf/2502.17516v1},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2502.17516},
}

@Article{Rai2024,
  author        = {Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
  title         = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  year          = {2024},
  month         = jul,
  abstract      = {Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2407.02646},
  eprint        = {2407.02646},
  file          = {:papers/Rai2024 - A Practical Review of Mechanistic Interpretability for Transformer Based Language Models.pdf:PDF:http\://arxiv.org/pdf/2407.02646v3},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, I.2.7},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2407.02646},
}

@Comment{jabref-meta: databaseType:bibtex;}
