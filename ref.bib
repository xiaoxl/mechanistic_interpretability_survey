@Article{Elhage2021,
  author  = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  journal = {Transformer Circuits Thread},
  title   = {A Mathematical Framework for Transformer Circuits},
  year    = {2021},
  url     = {https://transformer-circuits.pub/2021/framework/index.html},
}

@Article{Zhang2021,
  author    = {Zhang, Yu and Tino, Peter and Leonardis, Ales and Tang, Ke},
  journal   = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  title     = {A Survey on Neural Network Interpretability},
  year      = {2021},
  issn      = {2471-285X},
  month     = oct,
  number    = {5},
  pages     = {726–742},
  volume    = {5},
  doi       = {10.1109/tetci.2021.3100641},
  file      = {:papers/2012.14261v3.pdf:PDF},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  url       = {http://dx.doi.org/10.1109/TETCI.2021.3100641},
}

@InProceedings{Gantla2025,
  author    = {Gantla, Sandeep Reddy},
  booktitle = {2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)},
  title     = {Exploring Mechanistic Interpretability in Large Language Models: Challenges, Approaches, and Insights},
  year      = {2025},
  pages     = {1-8},
  doi       = {10.1109/ICDSAAI65575.2025.11011640},
  keywords  = {Surveys;Technological innovation;Large language models;Scalability;Transformers;Safety;Complexity theory;Integrated circuit modeling;Integrated circuit reliability;Biological neural networks;Mechanistic interpretability;deep learning;neural networks;transformers;interpretability;AI;challenges;future directions},
}

@Article{Ranaldi2025,
  author    = {Ranaldi, Leonardo},
  journal   = {Big Data and Cognitive Computing},
  title     = {Survey on the Role of Mechanistic Interpretability in Generative AI},
  year      = {2025},
  issn      = {2504-2289},
  month     = jul,
  number    = {8},
  pages     = {193},
  volume    = {9},
  doi       = {10.3390/bdcc9080193},
  file      = {:papers/BDCC-09-00193.pdf:PDF},
  publisher = {MDPI AG},
}

@Article{Golgoon2024,
  author        = {Golgoon, Ashkan and Filom, Khashayar and Kannan, Arjun Ravi},
  journal       = {5th ACM International Conference on AI in Finance (ICAIF 2024)},
  title         = {Mechanistic interpretability of large language models with applications to the financial services industry},
  year          = {2024},
  month         = nov,
  pages         = {660--668},
  abstract      = {Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and $11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant role in the task completion.},
  archiveprefix = {arXiv},
  booktitle     = {Proceedings of the 5th ACM International Conference on AI in Finance},
  collection    = {ICAIF ’24},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2024-07-15},
  doi           = {10.1145/3677052.3698612},
  eprint        = {2407.11215},
  file          = {:papers/Golgoon2024 - Mechanistic Interpretability of Large Language Models with Applications to the Financial Services Industry.pdf:PDF:http\://arxiv.org/pdf/2407.11215v2},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Engineering / Finance / Science (cs.CE), Computation and Language (cs.CL), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Mathematics, I.2.7, 68T01},
  primaryclass  = {cs.LG},
  publisher     = {ACM},
  series        = {ICAIF ’24},
}

@Article{Bereska2024,
  author        = {Bereska, Leonard and Gavves, Efstratios},
  title         = {Mechanistic Interpretability for AI Safety -- A Review},
  year          = {2024},
  month         = apr,
  abstract      = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2404.14082},
  eprint        = {2404.14082},
  file          = {:papers/Bereska2024 - Mechanistic Interpretability for AI Safety a Review.pdf:PDF:http\://arxiv.org/pdf/2404.14082v3},
  keywords      = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2404.14082},
}

@Article{Lin2025,
  author        = {Lin, Zihao and Basu, Samyadeep and Beigi, Mohammad and Manjunatha, Varun and Rossi, Ryan A. and Wang, Zichao and Zhou, Yufan and Balasubramanian, Sriram and Zarei, Arman and Rezaei, Keivan and Shen, Ying and Yao, Barry Menglong and Xu, Zhiyang and Liu, Qin and Zhang, Yuxiang and Sun, Yan and Liu, Shilong and Shen, Li and Li, Hongxuan and Feizi, Soheil and Huang, Lifu},
  title         = {A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models},
  year          = {2025},
  month         = feb,
  abstract      = {The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2502.17516},
  eprint        = {2502.17516},
  file          = {:papers/Lin2025 - A Survey on Mechanistic Interpretability for Multi Modal Foundation Models.pdf:PDF:http\://arxiv.org/pdf/2502.17516v1},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2502.17516},
}

@Article{Rai2024,
  author        = {Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
  title         = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  year          = {2024},
  month         = jul,
  abstract      = {Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we provide a comprehensive survey from a task-centric perspective, organizing the taxonomy of MI research around specific research questions or tasks. We outline the fundamental objects of study in MI, along with the techniques, evaluation methods, and key findings for each task in the taxonomy. In particular, we present a task-centric taxonomy as a roadmap for beginners to navigate the field by helping them quickly identify impactful problems in which they are most interested and leverage MI for their benefit. Finally, we discuss the current gaps in the field and suggest potential future directions for MI research.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2407.02646},
  eprint        = {2407.02646},
  file          = {:papers/Rai2024 - A Practical Review of Mechanistic Interpretability for Transformer Based Language Models.pdf:PDF:http\://arxiv.org/pdf/2407.02646v3},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, I.2.7},
  primaryclass  = {cs.AI},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2407.02646},
}

@InProceedings{Vig2020,
  author    = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {12388--12401},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
}

@InProceedings{Meng2022,
  author    = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
  title     = {Locating and editing factual associations in GPT},
  year      = {2022},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '22},
  abstract  = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difficult counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing.},
  articleno = {1262},
  groups    = {llm25},
  isbn      = {9781713871088},
  location  = {New Orleans, LA, USA},
  numpages  = {14},
}

@Article{Meng2022a,
  author        = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  title         = {Mass-Editing Memory in a Transformer},
  year          = {2022},
  month         = oct,
  abstract      = {Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2210.07229},
  eprint        = {2210.07229},
  groups        = {llm25},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{Nanda2022,
  author = {Neel Nanda and Joseph Bloom},
  title  = {TransformerLens: A library for mechanistic interpretability of GPT-style language models},
  year   = {2022},
  url    = {https://github.com/TransformerLensOrg/TransformerLens},
}

@Article{Kriegeskorte2008,
  author    = {Kriegeskorte, Nikolaus},
  journal   = {Frontiers in Systems Neuroscience},
  title     = {Representational similarity analysis – connecting the branches of systems neuroscience},
  year      = {2008},
  issn      = {1662-5137},
  doi       = {10.3389/neuro.06.004.2008},
  groups    = {llm25},
  publisher = {Frontiers Media SA},
}

@Article{Nili2014,
  author    = {Nili, Hamed and Wingfield, Cai and Walther, Alexander and Su, Li and Marslen-Wilson, William and Kriegeskorte, Nikolaus},
  journal   = {PLoS Computational Biology},
  title     = {A Toolbox for Representational Similarity Analysis},
  year      = {2014},
  issn      = {1553-7358},
  month     = apr,
  number    = {4},
  pages     = {e1003553},
  volume    = {10},
  doi       = {10.1371/journal.pcbi.1003553},
  editor    = {Prlic, Andreas},
  groups    = {llm25},
  publisher = {Public Library of Science (PLoS)},
}

@Article{Olah2020,
  author    = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal   = {Distill},
  title     = {Zoom In: An Introduction to Circuits},
  year      = {2020},
  issn      = {2476-0757},
  month     = mar,
  number    = {3},
  volume    = {5},
  doi       = {10.23915/distill.00024.001},
  groups    = {llm25},
  publisher = {Distill Working Group},
}

@Misc{Olah2022,
  author       = {Chris Olah},
  howpublished = {Transformer Circuits blog},
  month        = jun,
  note         = {Published June 27, 2022},
  title        = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
  year         = {2022},
  url          = {https://www.transformer-circuits.pub/2022/mech-interp-essay},
}

@Article{Zou2023,
  author        = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
  title         = {Representation Engineering: A Top-Down Approach to AI Transparency},
  year          = {2023},
  month         = oct,
  abstract      = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2310.01405},
  eprint        = {2310.01405},
  groups        = {llm25},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Computers and Society (cs.CY), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Wehner2025,
  author        = {Wehner, Jan and Abdelnabi, Sahar and Tan, Daniel and Krueger, David and Fritz, Mario},
  title         = {Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models},
  year          = {2025},
  month         = feb,
  abstract      = {Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2502.19649},
  eprint        = {2502.19649},
  groups        = {llm25},
  keywords      = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@InProceedings{Wei2022,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
  title     = {Chain-of-thought prompting elicits reasoning in large language models},
  year      = {2022},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '22},
  abstract  = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  articleno = {1800},
  isbn      = {9781713871088},
  location  = {New Orleans, LA, USA},
  numpages  = {14},
}

@Article{Turner2023,
  author        = {Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J. and Mini, Ulisse and MacDiarmid, Monte},
  title         = {Steering Language Models With Activation Engineering},
  year          = {2023},
  month         = aug,
  abstract      = {Prompt engineering and finetuning aim to maximize language model performance on a given metric (like toxicity reduction). However, these methods do not fully elicit a model's capabilities. To reduce this gap, we introduce activation engineering: the inference-time modification of activations in order to control (or steer) model outputs. Specifically, we introduce the Activation Addition (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as "Love" versus "Hate") to compute a steering vector (Subramani et al. 2022). By tactically adding in e.g. the "Love" - "Hate" steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level output properties (like topic and sentiment) while preserving performance on off-target tasks. ActAdd is lightweight: it does not require any machine optimization and works with a single pair of data points, which enables rapid iteration over steering. ActAdd demonstrates the power of activation engineering.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2308.10248},
  eprint        = {2308.10248},
  groups        = {llm25},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}
